<?xml version="1.0"?>

<config_machines version="2.0">
  <machine MACH="uconnhpc">
    <DESC>Oscar RHEL7, batch system is SLURM</DESC>
     <!--
    <NODENAME_REGEX>.*.storrs.hpc.brown.edu</NODENAME_REGEX>
     -->
    <NODENAME_REGEX>login*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/gpfs/homefs1/mok18003/CESMDATAROOT</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/gpfs/homefs1/mok18003/CESMDATAROOT/inputdata</DIN_LOC_ROOT>
     <!-- optional input location for clm forcing data -->
     <!--
    <DIN_LOC_ROOT_CLMFORC>/gpfs/data/shared/cesm/input_data/atm/datm7</DIN_LOC_ROOT_CLMFORC>
     -->
    <DOUT_S_ROOT>/gpfs/homefs1/$USER/data/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/gpfs/scratchfs1/$USER/ccsm_baselines</BASELINE_ROOT>
     <!--
    <CCSM_CPRNC>/gpfs/scratch/$USER/cesm_tools/ccsm_cprnc/cprnc</CCSM_CPRNC>
     -->

    <GMAKE>make</GMAKE>
    <GMAKE_J>8</GMAKE_J>

    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>Morphy Kuffour</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="pmi">--mpi=pmix</arg>
        <arg name="num_tasks"> -n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>

    <module_system type="module">
        <cmd_path lang="sh">module</cmd_path>
        <cmd_path lang="csh">module</cmd_path>
        <cmd_path lang="python">python</cmd_path>
        <cmd_path lang="perl">perl</cmd_path>
        <modules>
            <command name="load">python/3.10.5</command>
            <command name="load">cmake/3.23.2</command>
            <command name="load">pre-module</command>
            <command name="load">post-module</command>
            <command name="load">intel/oneapi/2022.3</command>
            <command name="load">zlib/1.2.12-ics</command>
            <command name="load">hdf5/1.13.2-ics</command>
            <command name="load">netcdf-fortran/4.6.0</command>
        </modules>
        <!-- <modules compiler="intel"> -->
        <!--     <command name="load">intel-compiler/2020.1.217</command> -->
        <!-- </modules> -->
        <modules mpilib="openmpi">
            <command name="load">openmpi</command>
        </modules>
        <!-- <modules mpilib="intelmpi"> -->
        <!--     <command name="load">intel-mpi/2020.1.217</command> -->
        <!-- </modules> -->
    </module_system>

    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="NETCDF_PATH">/gpfs/sharedfs1/admin/hpc2.0/apps/netcdf-fortran/4.6.0</env>
      <!-- <env name="PNETCDF_PATH">/gpfs/sharedfs1/admin/hpc2.0/apps/netcdf-fortran/4.6.0</env> -->
      <!-- <env name="MPICC">/cm/shared/apps/mpich/ge/gcc/64/3.4.2/bin/mpicc</env> -->
      <!-- <env name="MPIFC">/cm/shared/apps/mpich/ge/gcc/64/3.4.2/bin/mpif90</env> -->
      <!-- <env name="MKL_PATH">/gpfs/sharedfs1/admin/hpc2.0/apps/intel/oneapi/2022.3/mkl/2022.2.0</env> -->
    </environment_variables>
  </machine>
</config_machines>
